import cv2
import mediapipe as mp
import numpy as np
import joblib
import time
import threading
import tkinter as tk
from tkinter import messagebox
from PIL import Image, ImageTk

# ==============================
# üîπ 1. Load model v√† scaler
# ==============================
MODEL_PATH = 'model_mlp.pkl'
SCALER_PATH = 'scaler.pkl'

try:
    print("üîÑ ƒêang t·∫£i model v√† scaler...")
    model = joblib.load(MODEL_PATH)
    scaler = joblib.load(SCALER_PATH)
    print("‚úÖ ƒê√£ t·∫£i model v√† scaler!")
except Exception as e:
    print(f"‚ùå Kh√¥ng th·ªÉ t·∫£i model ho·∫∑c scaler: {e}")
    print("‚û°Ô∏è Vui l√≤ng ki·ªÉm tra l·∫°i phi√™n b·∫£n th∆∞ vi·ªán (numpy/scikit-learn) ho·∫∑c hu·∫•n luy·ªán l·∫°i model.")
    exit()

# ==============================
# üîπ 2. Kh·ªüi t·∫°o Mediapipe
# ==============================
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
# Cho ph√©p nh·∫≠n di·ªán c·∫£ 2 tay (nh∆∞ng logic b√™n d∆∞·ªõi ch·ªâ x·ª≠ l√Ω 1 tay)
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2,
                       min_detection_confidence=0.7, min_tracking_confidence=0.5)

# ==============================
# üîπ 3. Bi·∫øn ƒëi·ªÅu khi·ªÉn
# ==============================
sentence_raw = ""
last_detection_time = time.time()
last_recognition_time = 0
running = True
cap = None

# ==============================
# üîπ 4. H√†m x·ª≠ l√Ω Reset & Tho√°t
# ==============================
def reset_text():
    global sentence_raw
    sentence_raw = ""
    label_text.set("")
    print("\n--- K·∫æT QU·∫¢ ƒê√É ƒê∆Ø·ª¢C RESET ---")

def quit_app():
    global running
    running = False
    # ƒê·ª£i thread camera d·ª´ng h·∫≥n
    time.sleep(0.5) 
    if cap:
        cap.release()
    root.destroy()

# ==============================
# üîπ 5. H√†m x·ª≠ l√Ω Camera
# ==============================
def camera_loop():
    global last_detection_time, last_recognition_time, sentence_raw, cap

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        messagebox.showerror("L·ªói", "Kh√¥ng th·ªÉ m·ªü camera.")
        return

    while running:
        ret, frame = cap.read()
        if not ret or not running:
            break

        frame = cv2.flip(frame, 1)
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        rgb_frame.flags.writeable = False
        results = hands.process(rgb_frame)
        rgb_frame.flags.writeable = True

        current_time = time.time()

        if results.multi_hand_landmarks:
            # Ch·ªâ x·ª≠ l√Ω 1 tay (tay ƒë·∫ßu ti√™n ph√°t hi·ªán ƒë∆∞·ª£c)
            hand_landmarks = results.multi_hand_landmarks[0]
            
            # V·∫Ω skeleton l√™n frame (cho ph·∫ßn hi·ªÉn th·ªã b√™n tr√°i)
            drawing_frame = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR) 
            mp_drawing.draw_landmarks(
                drawing_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            # === üí° [ƒê√ÇY L√Ä PH·∫¶N S·ª¨A L·ªñI] ===
            # Chuy·ªÉn ƒë·ªïi to·∫° ƒë·ªô TUY·ªÜT ƒê·ªêI (t·ª´ camera)
            # sang to·∫° ƒë·ªô T∆Ø∆†NG ƒê·ªêI (so v·ªõi c·ªï tay)
            # ƒë·ªÉ kh·ªõp v·ªõi d·ªØ li·ªáu training (train.csv)
            
            all_landmarks_list = hand_landmarks.landmark
            
            # 2. L·∫•y to·∫° ƒë·ªô g·ªëc (c·ªï tay - ƒëi·ªÉm 0)
            base_x, base_y, base_z = all_landmarks_list[0].x, all_landmarks_list[0].y, all_landmarks_list[0].z

            landmarks_relative = []
            
            # 3. T√≠nh to·∫° ƒë·ªô t∆∞∆°ng ƒë·ªëi c·ªßa T·∫§T C·∫¢ 21 ƒëi·ªÉm
            for lm in all_landmarks_list:
                landmarks_relative.extend([lm.x - base_x, lm.y - base_y, lm.z - base_z])
            # ==================================

            # Logic qu√©t 1 gi√¢y 1 l·∫ßn
            if current_time - last_recognition_time >= 1.0:
                
                # 4. D√πng 'landmarks_relative' (63 features) ƒë·ªÉ d·ª± ƒëo√°n
                X_input = np.array(landmarks_relative).reshape(1, -1)
                
                X_scaled = scaler.transform(X_input)
                y_pred = model.predict(X_scaled)
                detected_letter = y_pred[0]
                last_recognition_time = current_time

                sentence_raw += detected_letter
                label_text.set(sentence_raw)
                
                print(f"C√¢u th√¥ (raw): {sentence_raw}")

            last_detection_time = time.time()
            
            # D√πng frame ƒë√£ v·∫Ω skeleton
            final_frame_for_gui = drawing_frame 

        else:
            # Kh√¥ng ph√°t hi·ªán tay
            final_frame_for_gui = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR) # D√πng frame g·ªëc
            
            # Logic th√™m d·∫•u c√°ch sau 1.5s
            if current_time - last_detection_time > 1.5:
                if len(sentence_raw) > 0 and not sentence_raw.endswith(" "):
                    sentence_raw += " "
                    label_text.set(sentence_raw)
                    
                    print(f"C√¢u th√¥ (raw): {sentence_raw}")
                    
                last_detection_time = current_time 

        # Convert ·∫£nh cho Tkinter hi·ªÉn th·ªã
        try:
            img = Image.fromarray(cv2.cvtColor(final_frame_for_gui, cv2.COLOR_BGR2RGB))
            img = img.resize((640, 480)) 
            imgtk = ImageTk.PhotoImage(image=img)

            # Hi·ªÉn th·ªã l√™n GUI
            video_label.imgtk = imgtk
            video_label.configure(image=imgtk)
        except Exception as e:
            # B·ªè qua l·ªói n·∫øu GUI ƒë√£ b·ªã ƒë√≥ng
            if running:
                print(f"L·ªói c·∫≠p nh·∫≠t GUI: {e}")


    if cap:
        cap.release()

# ==============================
# üîπ 6. Giao di·ªán Tkinter
# ==============================
root = tk.Tk()
root.title("Vietnamese Sign Language Recognition (Model 1)")

root.geometry("1280x480")
root.resizable(False, False)

# Khung tr√°i (camera) - 640x480
frame_left = tk.Frame(root, width=640, height=480, bg="black")
frame_left.pack(side="left", fill="both", expand=True)
frame_left.pack_propagate(False) 

video_label = tk.Label(frame_left, bg="black")
video_label.pack(fill="both", expand=True)

# Khung ph·∫£i (text + n√∫t) - 640x480
frame_right = tk.Frame(root, width=640, height=480, bg="#1E1E1E")
frame_right.pack(side="right", fill="both", expand=True)
frame_right.pack_propagate(False) 

# Label k·∫øt qu·∫£
label_title = tk.Label(frame_right, text="K·∫øt qu·∫£ nh·∫≠n di·ªán", font=("Arial", 18, "bold"), fg="white", bg="#1E1E1E")
label_title.pack(pady=(20, 10)) 

text_display_frame = tk.Frame(frame_right, bg="#1E1E1E", height=300, width=600)
text_display_frame.pack(pady=10)
text_display_frame.pack_propagate(False)

label_text = tk.StringVar()
label_display = tk.Label(text_display_frame, textvariable=label_text, font=("Consolas", 20), fg="#00FF00", bg="#1E1E1E", wraplength=580, justify="left", anchor="nw")
label_display.pack(fill="both", expand=True, padx=10)

# N√∫t Reset v√† Tho√°t
btn_frame = tk.Frame(frame_right, bg="#1E1E1E")
btn_frame.pack(pady=20) 

btn_reset = tk.Button(btn_frame, text="üîÅ Reset", command=reset_text, width=10, height=2, bg="#007ACC", fg="white", font=("Arial", 12, "bold"), relief="raised", borderwidth=2)
btn_reset.pack(side="left", padx=20)

btn_quit = tk.Button(btn_frame, text="‚ùå Tho√°t", command=quit_app, width=10, height=2, bg="#D9534F", fg="white", font=("Arial", 12, "bold"), relief="raised", borderwidth=2)
btn_quit.pack(side="right", padx=20)

# X·ª≠ l√Ω n√∫t X (WM_DELETE_WINDOW)
root.protocol("WM_DELETE_WINDOW", quit_app)

# H·ªèi b·∫≠t camera
if messagebox.askyesno("B·∫≠t camera", "üì∑ B·∫°n c√≥ cho ph√©p m·ªü camera ƒë·ªÉ nh·∫≠n di·ªán tay kh√¥ng?"):
    threading.Thread(target=camera_loop, daemon=True).start()
else:
    messagebox.showinfo("Tho√°t", "B·∫°n ƒë√£ t·ª´ ch·ªëi b·∫≠t camera. ·ª®ng d·ª•ng s·∫Ω ƒë√≥ng.")
    root.destroy()

# V√≤ng l·∫∑p GUI
if 'root' in locals() and root.winfo_exists():
    root.mainloop()

print("·ª®ng d·ª•ng ƒë√£ ƒë√≥ng.")